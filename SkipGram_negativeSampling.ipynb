{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5398a7-bf22-4299-9c3b-b8de416871a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe88bd8-59ea-4ed4-a6fd-eb03e472fd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CuPy version: 13.3.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using CuPy version: {cp.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491a6a81-d5ae-4af1-a3fb-0da6f0d42c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "window_size = 5  # Context window size\n",
    "num_negative_samples = 5  # Number of negative samples\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a03e6f-cdd2-46fd-a3b5-34dc9b17c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary from CSV\n",
    "vocab_df = pd.read_csv('vocabulary_main.csv')\n",
    "word_to_index = dict(zip(vocab_df['word'], vocab_df['index']))\n",
    "index_to_word = dict(zip(vocab_df['index'], vocab_df['word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aee937d-6bc3-4927-a8a3-f63b3b18d271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59419"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary size\n",
    "vocab_size = len(vocab_df)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37379d3-349b-4fd0-a32d-3305513b365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load skip-gram pairs from CSV\n",
    "pairs_df = pd.read_csv('pairs_main.csv')\n",
    "pairs = [(row['Center_Word_Index'], row['Context_Word_Index']) for _, row in pairs_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220a4de8-672e-4a75-b5e1-d64706056fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1653814 pairs (20% of the original dataset)\n"
     ]
    }
   ],
   "source": [
    "# Use only the first 20% of the dataset\n",
    "num_pairs = len(pairs)\n",
    "pairs = pairs[:int(0.2 * num_pairs)]\n",
    "print(f\"Training on {len(pairs)} pairs (20% of the original dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad146db-79c7-408e-91ed-6d64dc485f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming word frequencies are known for negative sampling\n",
    "word_freqs = cp.random.rand(vocab_size)  # Replace with actual word frequencies\n",
    "word_freqs = word_freqs / cp.sum(word_freqs)\n",
    "\n",
    "# Negative sampling distribution (raising to 3/4 power for better performance)\n",
    "neg_sample_probs = cp.power(word_freqs, 3/4)\n",
    "neg_sample_probs = neg_sample_probs / cp.sum(neg_sample_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050b4023-e254-47f9-8ffc-faa94be3b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram model definition\n",
    "class SkipGramModel:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.in_embeddings = cp.random.uniform(-1, 1, (vocab_size, embedding_dim)).astype(cp.float32)\n",
    "        self.out_embeddings = cp.random.uniform(-1, 1, (vocab_size, embedding_dim)).astype(cp.float32)\n",
    "\n",
    "    def forward(self, center_word, context_word, negative_samples):\n",
    "        center_embedding = self.in_embeddings[center_word]\n",
    "        context_embedding = self.out_embeddings[context_word]\n",
    "        negative_embedding = self.out_embeddings[negative_samples]\n",
    "\n",
    "        positive_score = cp.sum(center_embedding * context_embedding, axis=1)\n",
    "        positive_score = self.sigmoid(positive_score)\n",
    "\n",
    "        negative_score = cp.einsum('bnd,bd->bn', negative_embedding, center_embedding)\n",
    "        negative_score = self.sigmoid(-negative_score)\n",
    "\n",
    "        loss = -cp.log(positive_score + 1e-9) - cp.sum(cp.log(negative_score + 1e-9), axis=1)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "    def backward(self, center_word, context_word, negative_samples, learning_rate):\n",
    "        center_embedding = self.in_embeddings[center_word]\n",
    "        context_embedding = self.out_embeddings[context_word]\n",
    "        negative_embedding = self.out_embeddings[negative_samples]\n",
    "\n",
    "        positive_score = cp.sum(center_embedding * context_embedding, axis=1)\n",
    "        positive_score = self.sigmoid(positive_score)\n",
    "\n",
    "        negative_score = cp.einsum('bnd,bd->bn', negative_embedding, center_embedding)\n",
    "        negative_score = self.sigmoid(-negative_score)\n",
    "\n",
    "        # Gradients\n",
    "        grad_center = (positive_score - 1)[:, cp.newaxis] * context_embedding + \\\n",
    "                      cp.einsum('bn,bnd->bd', negative_score, negative_embedding)\n",
    "        grad_context = (positive_score - 1)[:, cp.newaxis] * center_embedding\n",
    "        grad_negative = cp.einsum('bn,bd->bnd', negative_score, center_embedding)\n",
    "\n",
    "        # Update embeddings\n",
    "        self.in_embeddings[center_word] -= learning_rate * grad_center\n",
    "        self.out_embeddings[context_word] -= learning_rate * grad_context\n",
    "        \n",
    "        # Use a loop to update negative samples\n",
    "        for i in range(negative_samples.shape[0]):\n",
    "            for j in range(negative_samples.shape[1]):\n",
    "                self.out_embeddings[negative_samples[i, j]] -= learning_rate * grad_negative[i, j]\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "# Function to get negative samples\n",
    "def get_negative_samples(batch_size, num_negative_samples, vocab_size, neg_sample_probs):\n",
    "    return cp.random.choice(vocab_size, size=(batch_size, num_negative_samples), p=neg_sample_probs.get()).astype(cp.int32)\n",
    "\n",
    "# Generate batches of data\n",
    "def generate_batches(pairs, batch_size):\n",
    "    random.shuffle(pairs)\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i:i + batch_size]\n",
    "        center_words = cp.array([pair[0] for pair in batch], dtype=cp.int32)\n",
    "        context_words = cp.array([pair[1] for pair in batch], dtype=cp.int32)\n",
    "        yield center_words, context_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c123b0b1-d786-43ba-8808-49babe6631ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 280795.1798, Time Taken: 793.60 seconds================] 100.0%\n",
      "Epoch 2, Loss: 383193.0775, Time Taken: 789.72 seconds================] 100.0%\n",
      "Epoch 3, Loss: 429941.8852, Time Taken: 787.89 seconds================] 100.0%\n",
      "Epoch 4, Loss: 412768.6777, Time Taken: 794.21 seconds================] 100.0%\n",
      "Epoch 5, Loss: 407874.4829, Time Taken: 796.32 seconds================] 100.0%\n",
      "Training completed and embeddings saved.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Training loop with timing and progress tracking\n",
    "total_batches = len(pairs) // batch_size + (1 if len(pairs) % batch_size != 0 else 0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time of the epoch\n",
    "    total_loss = 0\n",
    "    batches_processed = 0\n",
    "\n",
    "    for center_words, context_words in generate_batches(pairs, batch_size):\n",
    "        negative_samples = get_negative_samples(len(center_words), num_negative_samples, vocab_size, neg_sample_probs)\n",
    "\n",
    "        loss = model.forward(center_words, context_words, negative_samples)\n",
    "        model.backward(center_words, context_words, negative_samples, learning_rate)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batches_processed += 1\n",
    "\n",
    "        # Print progress\n",
    "        progress = batches_processed / total_batches\n",
    "        print(f\"\\rEpoch {epoch + 1}, Progress: [{('=' * int(50 * progress)):50s}] {progress:.1%}\", end=\"\")\n",
    "\n",
    "    end_time = time.time()  # End time of the epoch\n",
    "    epoch_time = end_time - start_time  # Time taken for this epoch\n",
    "\n",
    "    print(f\"\\rEpoch {epoch + 1}, Loss: {total_loss:.4f}, Time Taken: {epoch_time:.2f} seconds\")\n",
    "\n",
    "# Save embeddings\n",
    "cp.save('in_embeddings_skipgram_cupy.npy', model.in_embeddings)\n",
    "cp.save('out_embeddings_skipgram_cupy.npy', model.out_embeddings)\n",
    "\n",
    "print(\"Training completed and embeddings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b2386a9-8420-4a68-a316-ac0e26b1ee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing MRR for window size 2...\n",
      "Processed 0 test pairs, current MRR: 0.0004\n",
      "Processed 10000 test pairs, current MRR: 0.0008\n",
      "Processed 20000 test pairs, current MRR: 0.0007\n",
      "Processed 30000 test pairs, current MRR: 0.0007\n",
      "Processed 40000 test pairs, current MRR: 0.0008\n",
      "Processed 50000 test pairs, current MRR: 0.0008\n",
      "Processed 60000 test pairs, current MRR: 0.0009\n",
      "Processed 70000 test pairs, current MRR: 0.0009\n",
      "Processed 80000 test pairs, current MRR: 0.0008\n",
      "MRR for window size 2: 0.0008\n",
      "Computing MRR for window size 4...\n",
      "Processed 0 test pairs, current MRR: 0.0004\n",
      "Processed 10000 test pairs, current MRR: 0.0005\n",
      "Processed 20000 test pairs, current MRR: 0.0008\n",
      "Processed 30000 test pairs, current MRR: 0.0007\n",
      "Processed 40000 test pairs, current MRR: 0.0007\n",
      "Processed 50000 test pairs, current MRR: 0.0007\n",
      "Processed 60000 test pairs, current MRR: 0.0007\n",
      "Processed 70000 test pairs, current MRR: 0.0007\n",
      "Processed 80000 test pairs, current MRR: 0.0007\n",
      "Processed 90000 test pairs, current MRR: 0.0008\n",
      "Processed 100000 test pairs, current MRR: 0.0008\n",
      "Processed 110000 test pairs, current MRR: 0.0009\n",
      "Processed 120000 test pairs, current MRR: 0.0009\n",
      "Processed 130000 test pairs, current MRR: 0.0008\n",
      "Processed 140000 test pairs, current MRR: 0.0008\n",
      "Processed 150000 test pairs, current MRR: 0.0008\n",
      "MRR for window size 4: 0.0008\n",
      "Computing MRR for window size 5...\n",
      "Processed 0 test pairs, current MRR: 0.0004\n",
      "Processed 10000 test pairs, current MRR: 0.0005\n",
      "Processed 20000 test pairs, current MRR: 0.0006\n",
      "Processed 30000 test pairs, current MRR: 0.0008\n",
      "Processed 40000 test pairs, current MRR: 0.0007\n",
      "Processed 50000 test pairs, current MRR: 0.0007\n",
      "Processed 60000 test pairs, current MRR: 0.0007\n",
      "Processed 70000 test pairs, current MRR: 0.0007\n",
      "Processed 80000 test pairs, current MRR: 0.0007\n",
      "Processed 90000 test pairs, current MRR: 0.0007\n",
      "Processed 100000 test pairs, current MRR: 0.0007\n",
      "Processed 110000 test pairs, current MRR: 0.0008\n",
      "Processed 120000 test pairs, current MRR: 0.0008\n",
      "Processed 130000 test pairs, current MRR: 0.0009\n",
      "Processed 140000 test pairs, current MRR: 0.0009\n",
      "Processed 150000 test pairs, current MRR: 0.0009\n",
      "Processed 160000 test pairs, current MRR: 0.0008\n",
      "Processed 170000 test pairs, current MRR: 0.0008\n",
      "Processed 180000 test pairs, current MRR: 0.0008\n",
      "Processed 190000 test pairs, current MRR: 0.0008\n",
      "MRR for window size 5: 0.0008\n",
      "MRR calculations completed for all window sizes.\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load saved embeddings from skip-gram model (as numpy, then convert to cupy)\n",
    "in_embeddings = cp.asarray(np.load('in_embeddings_skipgram_cupy.npy'))\n",
    "out_embeddings = cp.asarray(np.load('out_embeddings_skipgram_cupy.npy'))\n",
    "\n",
    "# Function to compute cosine similarity between two vectors using cupy\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return cp.dot(vec1, vec2) / (cp.linalg.norm(vec1) * cp.linalg.norm(vec2))\n",
    "\n",
    "# Function to compute the rank of the true word in the sorted similarity list using cupy\n",
    "def compute_rank(pred_embedding, true_word_idx, out_embeddings):\n",
    "    similarities = cp.dot(out_embeddings, pred_embedding)\n",
    "    sorted_indices = cp.argsort(similarities)[::-1]  # Sort by descending similarity\n",
    "    rank = cp.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "# Function to compute Mean Reciprocal Rank (MRR) using cupy\n",
    "def compute_mrr(test_pairs, in_embeddings, out_embeddings):\n",
    "    total_mrr = 0\n",
    "    num_samples = len(test_pairs)\n",
    "\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx = row['Center_Word_Index']\n",
    "        context_word_idx = row['Context_Word_Index']\n",
    "\n",
    "        # Get the embedding for the center word from the GPU\n",
    "        center_embedding = in_embeddings[center_word_idx]\n",
    "\n",
    "        # Compute the rank of the true context word\n",
    "        rank = compute_rank(center_embedding, context_word_idx, out_embeddings)\n",
    "\n",
    "        # Add reciprocal rank to total MRR\n",
    "        total_mrr += 1 / rank\n",
    "\n",
    "        # Print progress every 10,000 samples\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} test pairs, current MRR: {total_mrr / (i + 1):.4f}\")\n",
    "\n",
    "    avg_mrr = total_mrr / num_samples\n",
    "    return avg_mrr\n",
    "\n",
    "# Load the test pairs for window size 2, 4, and 5\n",
    "pairs_test_2 = pd.read_csv('pairs_test_2.csv')\n",
    "pairs_test_4 = pd.read_csv('pairs_test_4.csv')\n",
    "pairs_test_5 = pd.read_csv('pairs_test_5.csv')\n",
    "\n",
    "# Subset to 20% of the data (you can change this to load the full data if needed)\n",
    "num_samples_2 = int(0.2 * len(pairs_test_2))\n",
    "num_samples_4 = int(0.2 * len(pairs_test_4))\n",
    "num_samples_5 = int(0.2 * len(pairs_test_5))\n",
    "\n",
    "pairs_test_2_subset = pairs_test_2.iloc[:num_samples_2]\n",
    "pairs_test_4_subset = pairs_test_4.iloc[:num_samples_4]\n",
    "pairs_test_5_subset = pairs_test_5.iloc[:num_samples_5]\n",
    "\n",
    "# Compute MRR for window size 2\n",
    "print(\"Computing MRR for window size 2...\")\n",
    "mrr_2 = compute_mrr(pairs_test_2_subset, in_embeddings, out_embeddings)\n",
    "print(f\"MRR for window size 2: {mrr_2:.4f}\")\n",
    "\n",
    "# Compute MRR for window size 4\n",
    "print(\"Computing MRR for window size 4...\")\n",
    "mrr_4 = compute_mrr(pairs_test_4_subset, in_embeddings, out_embeddings)\n",
    "print(f\"MRR for window size 4: {mrr_4:.4f}\")\n",
    "\n",
    "# Compute MRR for window size 5\n",
    "print(\"Computing MRR for window size 5...\")\n",
    "mrr_5 = compute_mrr(pairs_test_5_subset, in_embeddings, out_embeddings)\n",
    "print(f\"MRR for window size 5: {mrr_5:.4f}\")\n",
    "\n",
    "print(\"MRR calculations completed for all window sizes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
