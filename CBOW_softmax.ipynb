{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "160ae284-344b-42fc-828b-469354d44507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "context_size = 2  # Number of context words on each side\n",
    "max_context_length = context_size * 2  # Maximum context size\n",
    "\n",
    "# Load vocabulary and CBOW pairs\n",
    "vocabulary_df = pd.read_csv('vocabulary_main.csv')  # The vocabulary CSV generated earlier\n",
    "cbow_pairs_df = pd.read_csv('cbow_pairs.csv')  # The CBOW pairs CSV\n",
    "\n",
    "# Get vocabulary size and map words to indices\n",
    "vocab_size = len(vocabulary_df)\n",
    "word_to_index = dict(zip(vocabulary_df['word'], vocabulary_df['index']))\n",
    "\n",
    "# Initialize weights\n",
    "W1 = cp.random.randn(vocab_size, embedding_dim) * 0.01  # Input weights\n",
    "W2 = cp.random.randn(embedding_dim, vocab_size) * 0.01  # Output weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e590054-b81f-45af-b400-8cec4dea28bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [3561, 722, 3002, 33806, 3561, 0, 0, 0, 0, 0]\n",
       "1           [3002, 722, 3002, 33806, 3561, 782, 0, 0, 0, 0]\n",
       "2         [3002, 3561, 3002, 33806, 3561, 782, 308, 0, 0...\n",
       "3         [3002, 3561, 722, 33806, 3561, 782, 308, 3002,...\n",
       "4         [3002, 3561, 722, 3002, 3561, 782, 308, 3002, ...\n",
       "                                ...                        \n",
       "887767    [4954, 27, 17834, 8195, 41, 912, 73, 1309, 326...\n",
       "887768    [27, 17834, 8195, 41, 10321, 73, 1309, 3266, 0...\n",
       "887769    [17834, 8195, 41, 10321, 912, 1309, 3266, 0, 0...\n",
       "887770         [8195, 41, 10321, 912, 73, 3266, 0, 0, 0, 0]\n",
       "887771            [41, 10321, 912, 73, 1309, 0, 0, 0, 0, 0]\n",
       "Name: Context, Length: 887772, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_pairs_df['Context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b35aff1-4cff-4cf1-91f3-a886c67c4392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 500/1387\n",
      "Epoch 1/5, Batch 1000/1387\n",
      "Epoch 1/5, Batch 1387/1387\n",
      "Epoch 1/5 completed in 145.92 seconds\n",
      "Epoch 2/5, Batch 500/1387\n",
      "Epoch 2/5, Batch 1000/1387\n",
      "Epoch 2/5, Batch 1387/1387\n",
      "Epoch 2/5 completed in 145.92 seconds\n",
      "Epoch 3/5, Batch 500/1387\n",
      "Epoch 3/5, Batch 1000/1387\n",
      "Epoch 3/5, Batch 1387/1387\n",
      "Epoch 3/5 completed in 145.96 seconds\n",
      "Epoch 4/5, Batch 500/1387\n",
      "Epoch 4/5, Batch 1000/1387\n",
      "Epoch 4/5, Batch 1387/1387\n",
      "Epoch 4/5 completed in 145.96 seconds\n",
      "Epoch 5/5, Batch 500/1387\n",
      "Epoch 5/5, Batch 1000/1387\n",
      "Epoch 5/5, Batch 1387/1387\n",
      "Epoch 5/5 completed in 145.97 seconds\n",
      "Training completed and embeddings saved to 'word_embeddings.npy'\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp_x = cp.exp(x - cp.max(x))  # Subtract max for numerical stability\n",
    "    return exp_x / cp.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cbow_forward(context_indices):\n",
    "    # Average the context word embeddings\n",
    "    context_vectors = W1[context_indices]\n",
    "    hidden_layer = cp.mean(context_vectors, axis=0)  # Average context vectors\n",
    "\n",
    "    # Compute output layer scores\n",
    "    output_scores = cp.dot(hidden_layer, W2)\n",
    "    output_probabilities = softmax(output_scores.reshape(1, -1))\n",
    "\n",
    "    return output_probabilities, hidden_layer\n",
    "\n",
    "def cbow_backward(center_word_index, output_probabilities, hidden_layer):\n",
    "    # Create one-hot encoding for the target word\n",
    "    target = cp.zeros((1, vocab_size))\n",
    "    target[0, center_word_index] = 1\n",
    "\n",
    "    # Compute gradients\n",
    "    output_error = output_probabilities - target\n",
    "    dW2 = cp.dot(hidden_layer.reshape(-1, 1), output_error)  # Gradient for W2\n",
    "    dW1 = cp.dot(output_error, W2.T) * (1 / max_context_length)  # Gradient for W1\n",
    "\n",
    "    return dW1, dW2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate the number of batches for 10% of the data\n",
    "    num_batches = len(cbow_pairs_df) // batch_size + (1 if len(cbow_pairs_df) % batch_size != 0 else 0)\n",
    "    num_batches_10_percent = num_batches // 10  # Get the number of batches for the first 10%\n",
    "\n",
    "    for batch_num in range(num_batches_10_percent):\n",
    "        batch_start = batch_num * batch_size\n",
    "        batch = cbow_pairs_df.iloc[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            context_indices = eval(row['Context'])  # Convert string representation to list\n",
    "            center_word_index = row['Center_Word']\n",
    "\n",
    "            # Forward pass\n",
    "            output_probabilities, hidden_layer = cbow_forward(context_indices)\n",
    "\n",
    "            # Backward pass\n",
    "            dW1, dW2 = cbow_backward(center_word_index, output_probabilities, hidden_layer)\n",
    "\n",
    "            # Update weights\n",
    "            W1[context_indices] -= learning_rate * dW1\n",
    "            W2 -= learning_rate * dW2\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (batch_num + 1) % 500 == 0 or batch_num == num_batches_10_percent - 1:  # Print every 10 batches or the last batch\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_num + 1}/{num_batches_10_percent}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Save the learned embeddings\n",
    "embeddings = W1  # Final embeddings for words\n",
    "np.save('word_embeddings.npy', cp.asnumpy(embeddings))  # Convert to NumPy array and save\n",
    "\n",
    "print(\"Training completed and embeddings saved to 'word_embeddings.npy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "399b16ee-62ca-4810-a60a-65543b0be3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dda56427-bd21-4948-a242-db4d674be66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved embeddings\n",
    "word_embeddings = np.load('word_embeddings.npy')  # Update path as necessary\n",
    "vocabulary_df = pd.read_csv('vocabulary_main.csv')  # Ensure you have the vocabulary\n",
    "word_to_index = dict(zip(vocabulary_df['word'], vocabulary_df['index']))\n",
    "\n",
    "# Load test pairs\n",
    "test_pairs = pd.read_csv('pairs_test_2.csv')  # Ensure this file has 'Center_Word_Index' and 'Context_Word_Index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05b18a4e-b439-4588-a761-81b6e34448bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 20% of the test pairs\n",
    "num_test_pairs = len(test_pairs)\n",
    "first_20_percent_test_pairs = test_pairs.iloc[:num_test_pairs // 5]  # First 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed9337a0-dfa1-4c4c-9581-3b72778172c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_index):\n",
    "    \"\"\"Retrieve the embedding for a word index or return the embedding for <UNK> if unseen.\"\"\"\n",
    "    if word_index >= len(word_embeddings):  # If word_index is out of range\n",
    "        return word_embeddings[word_to_index['<UNK>']]  # Replace with <UNK> embedding\n",
    "    return word_embeddings[word_index]\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_rank(pred_embedding, true_word_idx):\n",
    "    \"\"\"Compute the rank of the true word in the sorted similarity list.\"\"\"\n",
    "    similarities = np.dot(word_embeddings, pred_embedding)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Sort by descending order\n",
    "    rank = np.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "def compute_mrr(test_pairs):\n",
    "    \"\"\"Compute Mean Reciprocal Rank (MRR) over all test pairs.\"\"\"\n",
    "    total_mrr = 0\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx, context_word_idx = row['Center_Word_Index'], row['Context_Word_Index']\n",
    "\n",
    "        # Get the target embedding (center word) using the index\n",
    "        center_embedding = get_embedding(center_word_idx)  # Handle unseen words\n",
    "\n",
    "        # Calculate the rank of the true context word using its index\n",
    "        rank = compute_rank(center_embedding, context_word_idx)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        mrr_i = 1 / rank\n",
    "        total_mrr += mrr_i\n",
    "        \n",
    "        # Print progress every 50000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processed {i} pairs, total MRR: {total_mrr:.4f}\")\n",
    "\n",
    "    # Final MRR score\n",
    "    avg_mrr = total_mrr / len(test_pairs)\n",
    "    return avg_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4922b443-01a1-46b4-bfc0-080a58e1ce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403482, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c6e36d8-3116-4fca-a579-5d3992bcd74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 pairs, total MRR: 0.0014\n",
      "Processed 1000 pairs, total MRR: 2.4936\n",
      "Processed 2000 pairs, total MRR: 6.8308\n",
      "Processed 3000 pairs, total MRR: 21.1767\n",
      "Processed 4000 pairs, total MRR: 31.6735\n",
      "Processed 5000 pairs, total MRR: 38.8989\n",
      "Processed 6000 pairs, total MRR: 51.0123\n",
      "Processed 7000 pairs, total MRR: 64.6896\n",
      "Processed 8000 pairs, total MRR: 77.6309\n",
      "Processed 9000 pairs, total MRR: 85.0523\n",
      "Processed 10000 pairs, total MRR: 92.1855\n",
      "Processed 11000 pairs, total MRR: 107.8881\n",
      "Processed 12000 pairs, total MRR: 112.5430\n",
      "Processed 13000 pairs, total MRR: 118.1966\n",
      "Processed 14000 pairs, total MRR: 129.0416\n",
      "Processed 15000 pairs, total MRR: 137.9220\n",
      "Processed 16000 pairs, total MRR: 143.0388\n",
      "Processed 17000 pairs, total MRR: 148.1727\n",
      "Processed 18000 pairs, total MRR: 151.4654\n",
      "Processed 19000 pairs, total MRR: 159.4704\n",
      "Processed 20000 pairs, total MRR: 176.3019\n",
      "Processed 21000 pairs, total MRR: 180.1070\n",
      "Processed 22000 pairs, total MRR: 193.1161\n",
      "Processed 23000 pairs, total MRR: 200.1734\n",
      "Processed 24000 pairs, total MRR: 210.5015\n",
      "Processed 25000 pairs, total MRR: 218.4446\n",
      "Processed 26000 pairs, total MRR: 226.2303\n",
      "Processed 27000 pairs, total MRR: 234.6727\n",
      "Processed 28000 pairs, total MRR: 244.1880\n",
      "Processed 29000 pairs, total MRR: 259.0691\n",
      "Processed 30000 pairs, total MRR: 269.8491\n",
      "Processed 31000 pairs, total MRR: 275.3943\n",
      "Processed 32000 pairs, total MRR: 284.9204\n",
      "Processed 33000 pairs, total MRR: 294.5141\n",
      "Processed 34000 pairs, total MRR: 310.4078\n",
      "Processed 35000 pairs, total MRR: 315.5548\n",
      "Processed 36000 pairs, total MRR: 321.9208\n",
      "Processed 37000 pairs, total MRR: 326.2865\n",
      "Processed 38000 pairs, total MRR: 336.8926\n",
      "Processed 39000 pairs, total MRR: 341.3643\n",
      "Processed 40000 pairs, total MRR: 349.9823\n",
      "Processed 41000 pairs, total MRR: 358.4486\n",
      "Processed 42000 pairs, total MRR: 365.0822\n",
      "Processed 43000 pairs, total MRR: 372.6374\n",
      "Processed 44000 pairs, total MRR: 378.9639\n",
      "Processed 45000 pairs, total MRR: 386.4135\n",
      "Processed 46000 pairs, total MRR: 392.9586\n",
      "Processed 47000 pairs, total MRR: 396.6952\n",
      "Processed 48000 pairs, total MRR: 402.9905\n",
      "Processed 49000 pairs, total MRR: 413.5531\n",
      "Processed 50000 pairs, total MRR: 416.8212\n",
      "Processed 51000 pairs, total MRR: 423.7784\n",
      "Processed 52000 pairs, total MRR: 433.1466\n",
      "Processed 53000 pairs, total MRR: 445.7672\n",
      "Processed 54000 pairs, total MRR: 454.1680\n",
      "Processed 55000 pairs, total MRR: 459.3767\n",
      "Processed 56000 pairs, total MRR: 467.2352\n",
      "Processed 57000 pairs, total MRR: 474.6098\n",
      "Processed 58000 pairs, total MRR: 477.7382\n",
      "Processed 59000 pairs, total MRR: 488.4921\n",
      "Processed 60000 pairs, total MRR: 494.6085\n",
      "Processed 61000 pairs, total MRR: 500.9461\n",
      "Processed 62000 pairs, total MRR: 509.7055\n",
      "Processed 63000 pairs, total MRR: 514.5890\n",
      "Processed 64000 pairs, total MRR: 517.5016\n",
      "Processed 65000 pairs, total MRR: 520.6246\n",
      "Processed 66000 pairs, total MRR: 529.6955\n",
      "Processed 67000 pairs, total MRR: 535.4245\n",
      "Processed 68000 pairs, total MRR: 542.5494\n",
      "Processed 69000 pairs, total MRR: 548.1526\n",
      "Processed 70000 pairs, total MRR: 561.0527\n",
      "Processed 71000 pairs, total MRR: 564.9927\n",
      "Processed 72000 pairs, total MRR: 569.7957\n",
      "Processed 73000 pairs, total MRR: 579.8726\n",
      "Processed 74000 pairs, total MRR: 582.3880\n",
      "Processed 75000 pairs, total MRR: 587.9590\n",
      "Processed 76000 pairs, total MRR: 598.1112\n",
      "Processed 77000 pairs, total MRR: 600.9184\n",
      "Processed 78000 pairs, total MRR: 609.6261\n",
      "Processed 79000 pairs, total MRR: 626.9396\n",
      "Processed 80000 pairs, total MRR: 642.3930\n",
      "MRR for the first 20% of the test data: 0.0080\n"
     ]
    }
   ],
   "source": [
    "# Run MRR computation on the first 20% of test pairs\n",
    "mrr_score = compute_mrr(first_20_percent_test_pairs)\n",
    "print(f'MRR for the first 20% of the test data: {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e5d7698-a947-4258-b13b-7179fbbfd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs_window_4 = pd.read_csv('pairs_test_4.csv')  # Ensure this file has 'Center_Word_Index' and 'Context_Word_Index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cad7eb5e-52fd-4732-8766-ecb7acd1a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 20% of the test pairs for window size 4\n",
    "num_test_pairs_window_4 = len(test_pairs_window_4)\n",
    "first_20_percent_test_pairs_window_4 = test_pairs_window_4.iloc[:num_test_pairs_window_4 // 5]  # First 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b8681a0-04a7-4df9-920e-861619f55142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_index):\n",
    "    \"\"\"Retrieve the embedding for a word index or return the embedding for <UNK> if unseen.\"\"\"\n",
    "    if word_index >= len(word_embeddings):  # If word_index is out of range\n",
    "        return word_embeddings[word_to_index['<UNK>']]  # Replace with <UNK> embedding\n",
    "    return word_embeddings[word_index]\n",
    "\n",
    "def compute_rank(pred_embedding, true_word_idx):\n",
    "    \"\"\"Compute the rank of the true word in the sorted similarity list.\"\"\"\n",
    "    similarities = np.dot(word_embeddings, pred_embedding)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Sort by descending order\n",
    "    rank = np.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "def compute_mrr(test_pairs):\n",
    "    \"\"\"Compute Mean Reciprocal Rank (MRR) over all test pairs.\"\"\"\n",
    "    total_mrr = 0\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx, context_word_idx = row['Center_Word_Index'], row['Context_Word_Index']\n",
    "\n",
    "        # Get the target embedding (center word) using the index\n",
    "        center_embedding = get_embedding(center_word_idx)  # Handle unseen words\n",
    "\n",
    "        # Calculate the rank of the true context word using its index\n",
    "        rank = compute_rank(center_embedding, context_word_idx)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        mrr_i = 1 / rank\n",
    "        total_mrr += mrr_i\n",
    "        \n",
    "        # Print progress every 50000 iterations\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Processed {i} pairs, total MRR: {total_mrr:.4f}\")\n",
    "\n",
    "    # Final MRR score\n",
    "    avg_mrr = total_mrr / len(test_pairs)\n",
    "    return avg_mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a02972f-ef35-444d-be27-330a139862b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 pairs, total MRR: 0.0014\n",
      "Processed 10000 pairs, total MRR: 75.6442\n",
      "Processed 20000 pairs, total MRR: 174.9479\n",
      "Processed 30000 pairs, total MRR: 259.6337\n",
      "Processed 40000 pairs, total MRR: 352.7514\n",
      "Processed 50000 pairs, total MRR: 447.0202\n",
      "Processed 60000 pairs, total MRR: 559.1271\n",
      "Processed 70000 pairs, total MRR: 653.8185\n",
      "Processed 80000 pairs, total MRR: 725.3170\n",
      "Processed 90000 pairs, total MRR: 794.6552\n",
      "Processed 100000 pairs, total MRR: 861.0494\n",
      "Processed 110000 pairs, total MRR: 950.8984\n",
      "Processed 120000 pairs, total MRR: 1013.3488\n",
      "Processed 130000 pairs, total MRR: 1074.8235\n",
      "Processed 140000 pairs, total MRR: 1150.1395\n",
      "Processed 150000 pairs, total MRR: 1220.4922\n",
      "MRR for the first 20% of the test data (window size 4): 0.0083\n"
     ]
    }
   ],
   "source": [
    "# Run MRR computation on the first 20% of test pairs for window size 4\n",
    "mrr_score_window_4 = compute_mrr(first_20_percent_test_pairs_window_4)\n",
    "print(f'MRR for the first 20% of the test data (window size 4): {mrr_score_window_4:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f39c7144-b21c-4aa2-88e9-037fffbe4066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976012, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test pairs for window size 5\n",
    "test_pairs_window_5 = pd.read_csv('pairs_test_5.csv')# Ensure this file has 'Center_Word_Index' and 'Context_Word_Index'\n",
    "test_pairs_window_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5346c13d-4ec6-4faf-90e9-3c21685eb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 20% of the test pairs for window size 5\n",
    "num_test_pairs_window_5 = len(test_pairs_window_5)\n",
    "first_20_percent_test_pairs_window_5 = test_pairs_window_5.iloc[:num_test_pairs_window_5 // 5]  # First 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9af627a3-6b56-4691-a0a0-ad651031b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_index):\n",
    "    \"\"\"Retrieve the embedding for a word index or return the embedding for <UNK> if unseen.\"\"\"\n",
    "    if word_index >= len(word_embeddings):  # If word_index is out of range\n",
    "        return word_embeddings[word_to_index['<UNK>']]  # Replace with <UNK> embedding\n",
    "    return word_embeddings[word_index]\n",
    "\n",
    "def compute_rank(pred_embedding, true_word_idx):\n",
    "    \"\"\"Compute the rank of the true word in the sorted similarity list.\"\"\"\n",
    "    similarities = np.dot(word_embeddings, pred_embedding)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Sort by descending order\n",
    "    rank = np.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "def compute_mrr(test_pairs):\n",
    "    \"\"\"Compute Mean Reciprocal Rank (MRR) over all test pairs.\"\"\"\n",
    "    total_mrr = 0\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx, context_word_idx = row['Center_Word_Index'], row['Context_Word_Index']\n",
    "\n",
    "        # Get the target embedding (center word) using the index\n",
    "        center_embedding = get_embedding(center_word_idx)  # Handle unseen words\n",
    "\n",
    "        # Calculate the rank of the true context word using its index\n",
    "        rank = compute_rank(center_embedding, context_word_idx)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        mrr_i = 1 / rank\n",
    "        total_mrr += mrr_i\n",
    "        \n",
    "        # Print progress every 50000 iterations\n",
    "        if i % 25000 == 0:\n",
    "            print(f\"Processed {i} pairs, total MRR: {total_mrr:.4f}\")\n",
    "\n",
    "    # Final MRR score\n",
    "    avg_mrr = total_mrr / len(test_pairs)\n",
    "    return avg_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2ae7644-9d57-44ac-899e-48c78729b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 pairs, total MRR: 0.0014\n",
      "Processed 25000 pairs, total MRR: 222.6207\n",
      "Processed 50000 pairs, total MRR: 440.0809\n",
      "Processed 75000 pairs, total MRR: 698.4106\n",
      "Processed 100000 pairs, total MRR: 912.9227\n",
      "Processed 125000 pairs, total MRR: 1082.6599\n",
      "Processed 150000 pairs, total MRR: 1281.1896\n",
      "Processed 175000 pairs, total MRR: 1446.0686\n",
      "MRR for the first 20% of the test data (window size 5): 0.0084\n"
     ]
    }
   ],
   "source": [
    "# Run MRR computation on the first 20% of test pairs for window size 5\n",
    "mrr_score_window_5 = compute_mrr(first_20_percent_test_pairs_window_5)\n",
    "print(f'MRR for the first 20% of the test data (window size 5): {mrr_score_window_5:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
