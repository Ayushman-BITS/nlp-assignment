{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb6aea-ca08-4881-905d-83877e6d8c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  88%|████████▊ | 113454/129205 [03:23<00:27, 573.19it/s, loss=9.74]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2/10:  62%|██████▏   | 80258/129205 [02:21<01:24, 577.07it/s, loss=8.89]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3/10:  36%|███▌      | 46722/129205 [01:22<02:21, 581.97it/s, loss=8.67]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 4/10:  10%|█         | 13158/129205 [00:22<03:26, 561.30it/s, loss=8.53]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 4/10:  83%|████████▎ | 106648/129205 [03:09<00:38, 586.77it/s, loss=8.5]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5/10:  39%|███▉      | 50642/129205 [01:29<02:14, 582.32it/s, loss=8.41]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5/10:  79%|███████▉  | 101755/129205 [02:59<00:47, 579.16it/s, loss=8.4]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 6/10:  52%|█████▏    | 67357/129205 [01:58<01:48, 569.50it/s, loss=8.33]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 7/10:  25%|██▌       | 32711/129205 [00:57<02:46, 578.32it/s, loss=8.26]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 7/10:  98%|█████████▊| 126817/129205 [03:41<00:04, 582.85it/s, loss=8.26]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 8/10:  71%|███████   | 91887/129205 [02:41<01:05, 573.56it/s, loss=8.21]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 9/10:  46%|████▌     | 59468/129205 [01:45<02:00, 580.04it/s, loss=8.17]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 10/10:  20%|█▉        | 25456/129205 [00:45<02:57, 584.18it/s, loss=8.13]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 10/10:  91%|█████████ | 117361/129205 [03:27<00:20, 577.40it/s, loss=8.13]"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os  # Import os for directory handling\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Load vocabulary and skipgram pairs\n",
    "vocabulary_df = pd.read_csv('vocabulary_main.csv')\n",
    "pairs_df = pd.read_csv('pairs_main.csv')\n",
    "\n",
    "# Get vocabulary size and map words to indices\n",
    "vocab_size = len(vocabulary_df)\n",
    "word_to_index = dict(zip(vocabulary_df['word'], vocabulary_df['index']))\n",
    "\n",
    "# Prepare skipgram pairs (center and context word indices)\n",
    "center_words = cp.asarray(pairs_df['Center_Word_Index'].values)\n",
    "context_words = cp.asarray(pairs_df['Context_Word_Index'].values)\n",
    "\n",
    "# Initialize word embeddings\n",
    "target_embeddings = (cp.random.randn(vocab_size, embedding_dim) * 0.01).astype(cp.float32)\n",
    "context_embeddings = (cp.random.randn(vocab_size, embedding_dim) * 0.01).astype(cp.float32)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    exp_x = cp.exp(x - cp.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(predicted, true_index):\n",
    "    return -cp.log(predicted[cp.arange(len(true_index)), true_index])\n",
    "\n",
    "# Cosine similarity for evaluation\n",
    "def cosine_similarity_gpu(a, b):\n",
    "    a_norm = cp.linalg.norm(a, axis=1)\n",
    "    b_norm = cp.linalg.norm(b, axis=1)\n",
    "    return cp.dot(a, b.T) / cp.outer(a_norm, b_norm)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_embeddings(embeddings, word_to_index, test_words):\n",
    "    test_indices = [word_to_index[word] for word in test_words if word in word_to_index]\n",
    "    test_embeddings = embeddings[test_indices]\n",
    "    similarities = cosine_similarity_gpu(test_embeddings, embeddings)\n",
    "    return similarities\n",
    "\n",
    "# Create 'results' directory if it doesn't exist\n",
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    indices = cp.arange(len(center_words))\n",
    "    cp.random.shuffle(indices)\n",
    "    center_words_shuffled = center_words[indices]\n",
    "    context_words_shuffled = context_words[indices]\n",
    "    \n",
    "    progress_bar = tqdm(range(0, len(center_words), batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i in progress_bar:\n",
    "        batch_center_words = center_words_shuffled[i:i+batch_size]\n",
    "        batch_context_words = context_words_shuffled[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        target_embeddings_batch = target_embeddings[batch_center_words]\n",
    "        context_scores = cp.dot(target_embeddings_batch, context_embeddings.T)\n",
    "        predicted_probs = softmax(context_scores)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(predicted_probs, batch_context_words)\n",
    "        total_loss += cp.sum(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        predicted_probs[cp.arange(len(batch_context_words)), batch_context_words] -= 1\n",
    "        d_context_embeddings = cp.dot(predicted_probs.T, target_embeddings_batch)\n",
    "        d_target_embeddings_batch = cp.dot(predicted_probs, context_embeddings)\n",
    "        \n",
    "        # Update embeddings\n",
    "        target_embeddings[batch_center_words] -= learning_rate * d_target_embeddings_batch\n",
    "        context_embeddings -= learning_rate * d_context_embeddings\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': total_loss.get() / (i + batch_size)})\n",
    "    \n",
    "    # Learning rate decay\n",
    "    learning_rate *= 0.9\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.get()/len(center_words)}')\n",
    "    \n",
    "    # Save intermediate results in the 'results' directory\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        cp.save(os.path.join(results_dir, f'target_embeddings_epoch_{epoch+1}.npy'), target_embeddings)\n",
    "        cp.save(os.path.join(results_dir, f'context_embeddings_epoch_{epoch+1}.npy'), context_embeddings)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'Total training time: {total_time:.2f} seconds')\n",
    "\n",
    "# Normalize embeddings\n",
    "target_embeddings /= cp.linalg.norm(target_embeddings, axis=1, keepdims=True)\n",
    "context_embeddings /= cp.linalg.norm(context_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Save final embeddings in the 'results' directory\n",
    "cp.save(os.path.join(results_dir, 'target_embeddings_final.npy'), target_embeddings)\n",
    "cp.save(os.path.join(results_dir, 'context_embeddings_final.npy'), context_embeddings)\n",
    "\n",
    "# Evaluate embeddings\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'paris', 'france']\n",
    "similarities = evaluate_embeddings(target_embeddings, word_to_index, test_words)\n",
    "\n",
    "print(\"Sample cosine similarities:\")\n",
    "for i, word in enumerate(test_words):\n",
    "    most_similar = cp.argsort(similarities[i])[-5:][::-1]\n",
    "    print(f\"{word}: {[list(word_to_index.keys())[idx] for idx in most_similar.get()]}\")\n",
    "\n",
    "print(\"Training completed and embeddings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cd670c-976c-410c-acb9-8654e091cd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cosine similarities:\n",
      "king: ['king', 'Hairan', 'assyrian', 'lord', 'Esarhaddon']\n",
      "queen: ['queen', 'Aribi', 'elkhunu', 'lord', 'Zabibe']\n",
      "man: ['man', 'wound', 'dusk', 'heterosexuality', 'FN']\n",
      "woman: ['woman', 'sexual', 'unmarried', 'Fornication', 'heterosexuality']\n",
      "light: ['light', 'shade', 'bactericidal', 'HID', 'diode']\n",
      "kick: ['kick', 'Kinzer', 'timeout', 'Brotzman', 'kicker']\n"
     ]
    }
   ],
   "source": [
    "# Evaluate embeddings\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'light', 'kick']\n",
    "similarities = evaluate_embeddings(target_embeddings, word_to_index, test_words)\n",
    "\n",
    "print(\"Sample cosine similarities:\")\n",
    "for i, word in enumerate(test_words):\n",
    "    # Get the number of words in the vocabulary (i.e., the size of similarities)\n",
    "    num_words_in_vocab = similarities.shape[1]\n",
    "\n",
    "    # Limit the number of most similar words to the available vocabulary size\n",
    "    most_similar = cp.argsort(similarities[i])[-min(5, num_words_in_vocab):][::-1]\n",
    "\n",
    "    # Get the actual words corresponding to the indices of the most similar ones\n",
    "    similar_words = [list(word_to_index.keys())[idx] for idx in most_similar.get()]\n",
    "\n",
    "    print(f\"{word}: {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a93ea8a6-e6af-4ef3-955e-2cbd1b50f5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Center_Word_Index</th>\n",
       "      <th>Context_Word_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Center_Word_Index  Context_Word_Index\n",
       "0               1097                 906\n",
       "1                906                1097\n",
       "2               1097                 906\n",
       "3               1097                2240\n",
       "4               1097                   6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_pairs = pd.read_csv('pairs_test_5.csv')\n",
    "test_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f891b3-9012-4241-9e9b-172ce7cbe9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403482"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load saved embeddings\n",
    "target_embeddings = np.load('results/target_embeddings_final.npy')\n",
    "context_embeddings = np.load('results/context_embeddings_final.npy')\n",
    "\n",
    "# Load test pairs (contains indices)\n",
    "test_pairs = pd.read_csv('pairs_test_2.csv')\n",
    "\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12859691-eeea-4cda-8066-0b4e9f7d1eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size 2, processed 0 pairs, total MRR: 0.0026\n",
      "Window size 2, processed 50000 pairs, total MRR: 88.6753\n",
      "Window size 2, processed 100000 pairs, total MRR: 199.7818\n",
      "Window size 2, processed 150000 pairs, total MRR: 278.3159\n",
      "Window size 2, processed 200000 pairs, total MRR: 381.1548\n",
      "Window size 2, processed 250000 pairs, total MRR: 476.8200\n",
      "Window size 2, processed 300000 pairs, total MRR: 574.8599\n",
      "Window size 2, processed 350000 pairs, total MRR: 686.6888\n",
      "Window size 2, processed 400000 pairs, total MRR: 791.1328\n",
      "MRR for the test data (window size 2): 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "window_size = 2  # Context size window\n",
    "embedding_dim = 100  # Make sure this matches with the model\n",
    "context_size = 2 * window_size  # Total context size\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_rank(pred_embedding, true_word_idx):\n",
    "    \"\"\"Compute the rank of the true word in the sorted similarity list.\"\"\"\n",
    "    similarities = np.dot(context_embeddings, pred_embedding)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Sort by descending order\n",
    "    rank = np.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "def compute_mrr(test_pairs, context_size=10):\n",
    "    \"\"\"Compute Mean Reciprocal Rank (MRR) over all test pairs.\"\"\"\n",
    "    total_mrr = 0\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx, context_word_idx = row['Center_Word_Index'], row['Context_Word_Index']\n",
    "\n",
    "        # Get the target embedding (center word) using the index\n",
    "        center_embedding = target_embeddings[center_word_idx]\n",
    "\n",
    "        # Calculate the rank of the true context word using its index\n",
    "        rank = compute_rank(center_embedding, context_word_idx)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        mrr_i = 1 / rank\n",
    "        total_mrr += mrr_i\n",
    "        if(i%50000 == 0):\n",
    "            print(f\"Window size {window_size}, processed {i} pairs, total MRR: {total_mrr:.4f}\")\n",
    "\n",
    "    # Final MRR score\n",
    "    avg_mrr = total_mrr / len(test_pairs)\n",
    "    return avg_mrr\n",
    "\n",
    "# Run MRR computation\n",
    "mrr_score = compute_mrr(test_pairs, context_size=context_size)\n",
    "print(f'MRR for the test data (window size 2): {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c62123-e630-4a48-80af-0f0ed1a74a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for the test data (window size 2): 0.0020\n"
     ]
    }
   ],
   "source": [
    "print(f'MRR for the test data (window size 2): {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afbb6339-e6d3-42fa-8278-795c79d8e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789074"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test pairs (contains indices)\n",
    "test_pairs = pd.read_csv('pairs_test_4.csv')\n",
    "\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0076e508-87b9-4bc2-8506-3e3eb4e9e733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size 4, processed 0 pairs, total MRR: 0.0026\n",
      "Window size 4, processed 50000 pairs, total MRR: 151.8821\n",
      "Window size 4, processed 100000 pairs, total MRR: 232.6650\n",
      "Window size 4, processed 150000 pairs, total MRR: 333.1237\n",
      "Window size 4, processed 200000 pairs, total MRR: 523.3636\n",
      "Window size 4, processed 250000 pairs, total MRR: 635.3796\n",
      "Window size 4, processed 300000 pairs, total MRR: 728.5030\n",
      "Window size 4, processed 350000 pairs, total MRR: 849.5308\n",
      "Window size 4, processed 400000 pairs, total MRR: 1010.0404\n",
      "Window size 4, processed 450000 pairs, total MRR: 1145.5435\n",
      "Window size 4, processed 500000 pairs, total MRR: 1239.9683\n",
      "Window size 4, processed 550000 pairs, total MRR: 1362.1226\n",
      "Window size 4, processed 600000 pairs, total MRR: 1522.6606\n",
      "Window size 4, processed 650000 pairs, total MRR: 1677.9166\n",
      "Window size 4, processed 700000 pairs, total MRR: 1841.1805\n",
      "Window size 4, processed 750000 pairs, total MRR: 1978.5968\n",
      "MRR for the test data (window size 2): 0.0026\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "window_size = 4  # Context size window\n",
    "embedding_dim = 100  # Make sure this matches with the model\n",
    "context_size = 2 * window_size  # Total context size\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_rank(pred_embedding, true_word_idx):\n",
    "    \"\"\"Compute the rank of the true word in the sorted similarity list.\"\"\"\n",
    "    similarities = np.dot(context_embeddings, pred_embedding)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Sort by descending order\n",
    "    rank = np.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "def compute_mrr(test_pairs, context_size=10):\n",
    "    \"\"\"Compute Mean Reciprocal Rank (MRR) over all test pairs.\"\"\"\n",
    "    total_mrr = 0\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx, context_word_idx = row['Center_Word_Index'], row['Context_Word_Index']\n",
    "\n",
    "        # Get the target embedding (center word) using the index\n",
    "        center_embedding = target_embeddings[center_word_idx]\n",
    "\n",
    "        # Calculate the rank of the true context word using its index\n",
    "        rank = compute_rank(center_embedding, context_word_idx)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        mrr_i = 1 / rank\n",
    "        total_mrr += mrr_i\n",
    "        if(i%50000 == 0):\n",
    "            print(f\"Window size {window_size}, processed {i} pairs, total MRR: {total_mrr:.4f}\")\n",
    "\n",
    "    # Final MRR score\n",
    "    avg_mrr = total_mrr / len(test_pairs)\n",
    "    return avg_mrr\n",
    "\n",
    "# Run MRR computation\n",
    "mrr_score = compute_mrr(test_pairs, context_size=context_size)\n",
    "print(f'MRR for the test data (window size 4): {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19c2285a-47b0-45f1-b214-9af044ac7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for the test data (window size 4): 0.0026\n"
     ]
    }
   ],
   "source": [
    "print(f'MRR for the test data (window size 4): 0.0026')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17d6a8ac-cf54-44df-8c2d-379e329facb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976012"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test pairs (contains indices)\n",
    "test_pairs = pd.read_csv('pairs_test_5.csv')\n",
    "\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdea9217-f7d9-4d08-8699-a3adb9f57789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size 5, processed 0 pairs, total MRR: 0.0026\n",
      "Window size 5, processed 50000 pairs, total MRR: 165.7831\n",
      "Window size 5, processed 100000 pairs, total MRR: 269.6007\n",
      "Window size 5, processed 150000 pairs, total MRR: 349.8160\n",
      "Window size 5, processed 200000 pairs, total MRR: 498.7737\n",
      "Window size 5, processed 250000 pairs, total MRR: 704.7958\n",
      "Window size 5, processed 300000 pairs, total MRR: 837.4665\n",
      "Window size 5, processed 350000 pairs, total MRR: 952.3520\n",
      "Window size 5, processed 400000 pairs, total MRR: 1066.4647\n",
      "Window size 5, processed 450000 pairs, total MRR: 1193.5025\n",
      "Window size 5, processed 500000 pairs, total MRR: 1343.2588\n",
      "Window size 5, processed 550000 pairs, total MRR: 1503.7496\n",
      "Window size 5, processed 600000 pairs, total MRR: 1616.0950\n",
      "Window size 5, processed 650000 pairs, total MRR: 1724.7557\n",
      "Window size 5, processed 700000 pairs, total MRR: 1877.9371\n",
      "Window size 5, processed 750000 pairs, total MRR: 2070.0777\n",
      "Window size 5, processed 800000 pairs, total MRR: 2237.3418\n",
      "Window size 5, processed 850000 pairs, total MRR: 2450.6266\n",
      "Window size 5, processed 900000 pairs, total MRR: 2593.0756\n",
      "Window size 5, processed 950000 pairs, total MRR: 2733.7901\n",
      "MRR for the test data (window size 2): 0.0029\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "window_size = 5  # Context size window\n",
    "embedding_dim = 100  # Make sure this matches with the model\n",
    "context_size = 2 * window_size  # Total context size\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def compute_rank(pred_embedding, true_word_idx):\n",
    "    \"\"\"Compute the rank of the true word in the sorted similarity list.\"\"\"\n",
    "    similarities = np.dot(context_embeddings, pred_embedding)\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # Sort by descending order\n",
    "    rank = np.where(sorted_indices == true_word_idx)[0][0] + 1  # +1 to make it 1-based\n",
    "    return rank\n",
    "\n",
    "def compute_mrr(test_pairs, context_size=10):\n",
    "    \"\"\"Compute Mean Reciprocal Rank (MRR) over all test pairs.\"\"\"\n",
    "    total_mrr = 0\n",
    "    for i, row in test_pairs.iterrows():\n",
    "        center_word_idx, context_word_idx = row['Center_Word_Index'], row['Context_Word_Index']\n",
    "\n",
    "        # Get the target embedding (center word) using the index\n",
    "        center_embedding = target_embeddings[center_word_idx]\n",
    "\n",
    "        # Calculate the rank of the true context word using its index\n",
    "        rank = compute_rank(center_embedding, context_word_idx)\n",
    "        \n",
    "        # Calculate reciprocal rank\n",
    "        mrr_i = 1 / rank\n",
    "        total_mrr += mrr_i\n",
    "        if(i%50000 == 0):\n",
    "            print(f\"Window size {window_size}, processed {i} pairs, total MRR: {total_mrr:.4f}\")\n",
    "\n",
    "    # Final MRR score\n",
    "    avg_mrr = total_mrr / len(test_pairs)\n",
    "    return avg_mrr\n",
    "\n",
    "# Run MRR computation\n",
    "mrr_score = compute_mrr(test_pairs, context_size=context_size)\n",
    "print(f'MRR for the test data (window size 5): {mrr_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e1e75b-a140-4337-b4ed-b70bc1186003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for the test data (window size 5): 0.0029\n"
     ]
    }
   ],
   "source": [
    "print(f'MRR for the test data (window size 5): {mrr_score:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
